# Optimized-Response-Generation-with-LLaMA-2-Using-PQT-Techniques
The quantization technique used in your project is Post-Quantization Training (PQT), specifically 4-bit quantization, which is applied after the model is trained. This technique is used to reduce the model size and improve memory efficiency without needing to retrain the model from scratch. 
